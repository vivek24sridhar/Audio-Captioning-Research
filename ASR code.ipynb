{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Q1_EikUNXkN"
      },
      "outputs": [],
      "source": [
        "# This cell installs all required libraries for the project.\n",
        "# TensorFlow and TensorFlow Hub are used for loading and running\n",
        "# pre-trained deep learning models such as YAMNet.\n",
        "# Librosa and SoundFile are used for audio loading and processing.\n",
        "# OpenAI Whisper is used to convert human speech in audio into text.\n",
        "!pip install -q tensorflow tensorflow-hub librosa soundfile openai-whisper\n",
        "#Installs all required libraries for audio processing, deep learning, and speech recognition."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SoMjo_uaNXm1"
      },
      "outputs": [],
      "source": [
        "# This cell imports all required libraries into the program.\n",
        "# Import TensorFlow and TensorFlow Hub are used for running pre-trained models.\n",
        "# Import Librosa, SoundFile,and NumPy are used for audio loading and processing.\n",
        "# Import Counter is used to count dominant sound events.\n",
        "# Import Whisper is used for converting human speech in audio into text.\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import librosa\n",
        "import numpy as np\n",
        "import soundfile as sf\n",
        "from collections import Counter\n",
        "import whisper\n",
        "#Installs all required libraries for audio processing, deep learning, and speech recognition.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t8NcGYMzNXpo"
      },
      "outputs": [],
      "source": [
        "# This cell loads the pre-trained YAMNet model for environmental sound detection.\n",
        "# It extracts the list of sound class names used by YAMNet.\n",
        "# It also loads the Whisper model for converting speech audio into text.\n",
        "yamnet_model = hub.load(\"https://tfhub.dev/google/yamnet/1\")\n",
        "class_map_path = yamnet_model.class_map_path().numpy().decode(\"utf-8\")\n",
        "\n",
        "class_names = []\n",
        "with open(class_map_path) as f:\n",
        "    for line in f.readlines()[1:]:\n",
        "        class_names.append(line.strip().split(\",\")[2])\n",
        "\n",
        "whisper_model = whisper.load_model(\"tiny\")\n",
        "#Imports the necessary Python libraries used throughout the audio captioning pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "vK0RAPAQNXsi",
        "outputId": "2db82dcd-ee31-482d-a8ce-bf3f8e65a346"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Use the buttons below. Max 60 seconds.\n"
          ]
        },
        {
          "data": {
            "application/javascript": "\nconst b2text = blob => new Promise(resolve => {\n  const reader = new FileReader();\n  reader.onloadend = e => resolve(e.srcElement.result);\n  reader.readAsDataURL(blob);\n});\n\nvar record = maxTime => new Promise(async resolve => {\n  // Container\n  const container = document.createElement(\"div\");\n  container.style.marginTop = \"8px\";\n  container.style.fontSize = \"14px\";\n\n  // Countdown text\n  const label = document.createElement(\"div\");\n  label.style.fontWeight = \"bold\";\n  label.innerText = \"Ready.\";\n  container.appendChild(label);\n\n  // Buttons\n  const startBtn = document.createElement(\"button\");\n  startBtn.textContent = \"Start\";\n  startBtn.style.marginRight = \"4px\";\n\n  const pauseBtn = document.createElement(\"button\");\n  pauseBtn.textContent = \"Pause\";\n  pauseBtn.disabled = true;\n  pauseBtn.style.marginRight = \"4px\";\n\n  const resumeBtn = document.createElement(\"button\");\n  resumeBtn.textContent = \"Resume\";\n  resumeBtn.disabled = true;\n  resumeBtn.style.marginRight = \"4px\";\n\n  const stopBtn = document.createElement(\"button\");\n  stopBtn.textContent = \"Stop\";\n  stopBtn.disabled = true;\n\n  container.appendChild(startBtn);\n  container.appendChild(pauseBtn);\n  container.appendChild(resumeBtn);\n  container.appendChild(stopBtn);\n\n  document.querySelector(\"#output-area\").appendChild(container);\n\n  let stream = null;\n  let recorder = null;\n  let chunks = [];\n  let elapsed = 0;\n  let intervalId = null;\n\n  function updateLabel() {\n    const remaining = Math.max(0, Math.round(maxTime - elapsed));\n    label.innerText = \"Recording... \" + remaining + \" s left\";\n  }\n\n  function clearTimer() {\n    if (intervalId !== null) {\n      clearInterval(intervalId);\n      intervalId = null;\n    }\n  }\n\n  function startTimer() {\n    clearTimer();\n    intervalId = setInterval(() => {\n      elapsed += 1;\n      updateLabel();\n      if (elapsed >= maxTime) {\n        stopRecording();\n      }\n    }, 1000);\n  }\n\n  function stopRecording() {\n    clearTimer();\n    if (recorder && recorder.state !== \"inactive\") {\n      recorder.stop();\n    }\n    if (stream) {\n      stream.getTracks().forEach(t => t.stop());\n    }\n    startBtn.disabled = true;\n    pauseBtn.disabled = true;\n    resumeBtn.disabled = true;\n    stopBtn.disabled = true;\n  }\n\n  startBtn.onclick = async () => {\n    startBtn.disabled = true;\n    pauseBtn.disabled = false;\n    stopBtn.disabled = false;\n\n    stream = await navigator.mediaDevices.getUserMedia({ audio: true });\n    recorder = new MediaRecorder(stream);\n    chunks = [];\n\n    recorder.ondataavailable = e => chunks.push(e.data);\n\n    recorder.onstop = async () => {\n      const blob = new Blob(chunks);\n      const text = await b2text(blob);\n      label.innerText = \"Recording finished.\";\n      resolve(text);\n    };\n\n    recorder.start();\n    elapsed = 0;\n    updateLabel();\n    startTimer();\n  };\n\n  pauseBtn.onclick = () => {\n    if (recorder && recorder.state === \"recording\") {\n      recorder.pause();\n      pauseBtn.disabled = true;\n      resumeBtn.disabled = false;\n      label.innerText = \"Paused at \" + Math.round(elapsed) + \" s\";\n      clearTimer();\n    }\n  };\n\n  resumeBtn.onclick = () => {\n    if (recorder && recorder.state === \"paused\") {\n      recorder.resume();\n      pauseBtn.disabled = false;\n      resumeBtn.disabled = true;\n      updateLabel();\n      startTimer();\n    }\n  };\n\n  stopBtn.onclick = () => {\n    stopRecording();\n  };\n});\n",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# This cell allows the user to either upload an audio file or record audio live using the microphone.\n",
        "# The recorded or uploaded audio is saved and used as input for further processing.\n",
        "\n",
        "# Import required modules for file upload, JavaScript-based audio recording, and audio decoding\n",
        "from google.colab import files, output\n",
        "from IPython.display import Javascript\n",
        "from base64 import b64decode\n",
        "\n",
        "# Maximum allowed duration for live audio recording (in seconds)\n",
        "MAX_SEC = 10  # hard limit in seconds\n",
        "\n",
        "# JavaScript code for recording audio directly from the browser with start, pause, resume, and stop controls\n",
        "RECORD_JS = f\"\"\"\n",
        "const b2text = blob => new Promise(resolve => {{\n",
        "  const reader = new FileReader();\n",
        "  reader.onloadend = e => resolve(e.srcElement.result);\n",
        "  reader.readAsDataURL(blob);\n",
        "}});\n",
        "\n",
        "var record = maxTime => new Promise(async resolve => {{\n",
        "  // Container\n",
        "  const container = document.createElement(\"div\");\n",
        "  container.style.marginTop = \"8px\";\n",
        "  container.style.fontSize = \"14px\";\n",
        "\n",
        "  // Countdown text\n",
        "  const label = document.createElement(\"div\");\n",
        "  label.style.fontWeight = \"bold\";\n",
        "  label.innerText = \"Ready.\";\n",
        "  container.appendChild(label);\n",
        "\n",
        "  // Buttons\n",
        "  const startBtn = document.createElement(\"button\");\n",
        "  startBtn.textContent = \"Start\";\n",
        "  startBtn.style.marginRight = \"4px\";\n",
        "\n",
        "  const pauseBtn = document.createElement(\"button\");\n",
        "  pauseBtn.textContent = \"Pause\";\n",
        "  pauseBtn.disabled = true;\n",
        "  pauseBtn.style.marginRight = \"4px\";\n",
        "\n",
        "  const resumeBtn = document.createElement(\"button\");\n",
        "  resumeBtn.textContent = \"Resume\";\n",
        "  resumeBtn.disabled = true;\n",
        "  resumeBtn.style.marginRight = \"4px\";\n",
        "\n",
        "  const stopBtn = document.createElement(\"button\");\n",
        "  stopBtn.textContent = \"Stop\";\n",
        "  stopBtn.disabled = true;\n",
        "\n",
        "  container.appendChild(startBtn);\n",
        "  container.appendChild(pauseBtn);\n",
        "  container.appendChild(resumeBtn);\n",
        "  container.appendChild(stopBtn);\n",
        "\n",
        "  document.querySelector(\"#output-area\").appendChild(container);\n",
        "\n",
        "  let stream = null;\n",
        "  let recorder = null;\n",
        "  let chunks = [];\n",
        "  let elapsed = 0;\n",
        "  let intervalId = null;\n",
        "\n",
        "  function updateLabel() {{\n",
        "    const remaining = Math.max(0, Math.round(maxTime - elapsed));\n",
        "    label.innerText = \"Recording... \" + remaining + \" s left\";\n",
        "  }}\n",
        "\n",
        "  function clearTimer() {{\n",
        "    if (intervalId !== null) {{\n",
        "      clearInterval(intervalId);\n",
        "      intervalId = null;\n",
        "    }}\n",
        "  }}\n",
        "\n",
        "  function startTimer() {{\n",
        "    clearTimer();\n",
        "    intervalId = setInterval(() => {{\n",
        "      elapsed += 1;\n",
        "      updateLabel();\n",
        "      if (elapsed >= maxTime) {{\n",
        "        stopRecording();\n",
        "      }}\n",
        "    }}, 1000);\n",
        "  }}\n",
        "\n",
        "  function stopRecording() {{\n",
        "    clearTimer();\n",
        "    if (recorder && recorder.state !== \"inactive\") {{\n",
        "      recorder.stop();\n",
        "    }}\n",
        "    if (stream) {{\n",
        "      stream.getTracks().forEach(t => t.stop());\n",
        "    }}\n",
        "    startBtn.disabled = true;\n",
        "    pauseBtn.disabled = true;\n",
        "    resumeBtn.disabled = true;\n",
        "    stopBtn.disabled = true;\n",
        "  }}\n",
        "\n",
        "  startBtn.onclick = async () => {{\n",
        "    startBtn.disabled = true;\n",
        "    pauseBtn.disabled = false;\n",
        "    stopBtn.disabled = false;\n",
        "\n",
        "    stream = await navigator.mediaDevices.getUserMedia({{ audio: true }});\n",
        "    recorder = new MediaRecorder(stream);\n",
        "    chunks = [];\n",
        "\n",
        "    recorder.ondataavailable = e => chunks.push(e.data);\n",
        "\n",
        "    recorder.onstop = async () => {{\n",
        "      const blob = new Blob(chunks);\n",
        "      const text = await b2text(blob);\n",
        "      label.innerText = \"Recording finished.\";\n",
        "      resolve(text);\n",
        "    }};\n",
        "\n",
        "    recorder.start();\n",
        "    elapsed = 0;\n",
        "    updateLabel();\n",
        "    startTimer();\n",
        "  }};\n",
        "\n",
        "  pauseBtn.onclick = () => {{\n",
        "    if (recorder && recorder.state === \"recording\") {{\n",
        "      recorder.pause();\n",
        "      pauseBtn.disabled = true;\n",
        "      resumeBtn.disabled = false;\n",
        "      label.innerText = \"Paused at \" + Math.round(elapsed) + \" s\";\n",
        "      clearTimer();\n",
        "    }}\n",
        "  }};\n",
        "\n",
        "  resumeBtn.onclick = () => {{\n",
        "    if (recorder && recorder.state === \"paused\") {{\n",
        "      recorder.resume();\n",
        "      pauseBtn.disabled = false;\n",
        "      resumeBtn.disabled = true;\n",
        "      updateLabel();\n",
        "      startTimer();\n",
        "    }}\n",
        "  }};\n",
        "\n",
        "  stopBtn.onclick = () => {{\n",
        "    stopRecording();\n",
        "  }};\n",
        "}});\n",
        "\"\"\"\n",
        "\n",
        "# Python function to trigger browser-based audio recording and save the recorded audio as a WAV file\n",
        "def record_audio(sec=5, fname=\"recorded.wav\"):\n",
        "    if sec > MAX_SEC:\n",
        "        sec = MAX_SEC\n",
        "    print(f\"Use the buttons below. Max {sec} seconds.\")\n",
        "    display(Javascript(RECORD_JS))\n",
        "    s = output.eval_js(f\"record({sec})\")  # sec in seconds\n",
        "    b = b64decode(s.split(\",\")[1])\n",
        "    with open(fname, \"wb\") as f:\n",
        "        f.write(b)\n",
        "    print(\"Done recording.\")\n",
        "    return fname\n",
        "\n",
        "# ---- MAIN CHOICE CELL ----\n",
        "# Ask the user to choose between uploading a local audio file or recording live audio\n",
        "choice = input(\"Enter 1 for local audio, 2 for live recording: \").strip()\n",
        "\n",
        "if choice == \"1\":\n",
        "  # Option 1: Upload an existing audio file from the local system\n",
        "    print(\"Upload a local audio file...\")\n",
        "    uploaded = files.upload()\n",
        "    audio_path = list(uploaded.keys())[0]\n",
        "    print(\"Using local file:\", audio_path)\n",
        "\n",
        "elif choice == \"2\":\n",
        "  # Option 2: Record audio in real time using the system microphone\n",
        "    audio_path = record_audio(sec=MAX_SEC, fname=\"live_audio.wav\")\n",
        "    print(\"Using recorded file:\", audio_path)\n",
        "\n",
        "else:\n",
        "  # Handle invalid user input\n",
        "    raise ValueError(\"Invalid choice. Please run the cell again and enter 1 or 2.\")\n",
        "2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uryvazKINXvK",
        "outputId": "13b53e06-bbb9-449b-a4ab-cd6e69dfcd56"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-2435650416.py:4: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  y, sr = librosa.load(audio_path, sr=16000)\n",
            "/usr/local/lib/python3.12/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"
          ]
        }
      ],
      "source": [
        "# This cell loads the uploaded audio file and converts it into\n",
        "# a TensorFlow tensor so it can be processed by deep learning models.\n",
        "\n",
        "y, sr = librosa.load(audio_path, sr=16000)\n",
        "waveform = tf.convert_to_tensor(y, dtype=tf.float32)\n",
        "#Allows the user to upload an audio file and stores its path for processing.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dWXb8Z3bNXyd",
        "outputId": "a2c5e291-4181-4546-ac97-59d0ae1b5f20"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sample frame-level events: ['Speech', 'Music', 'Music', 'Music', 'Music', 'Music', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech']\n"
          ]
        }
      ],
      "source": [
        "# This cell runs the YAMNet model on the audio waveform to detect\n",
        "# sound events at the frame level and identifies the most probable\n",
        "# sound class for each audio frame.\n",
        "\n",
        "# YAMNet inference\n",
        "scores, embeddings, spectrogram = yamnet_model(waveform)\n",
        "\n",
        "# Frame-level top class detection\n",
        "frame_top_classes = tf.argmax(scores, axis=1).numpy()\n",
        "frame_event_labels = [class_names[i] for i in frame_top_classes]\n",
        "\n",
        "print(\"Sample frame-level events:\", frame_event_labels[:20])\n",
        "\n",
        "#Loads the uploaded audio file and converts it into a TensorFlow tensor for model inference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Od9y_Pg5NX1V",
        "outputId": "12e5595b-387c-4704-b05b-e4900bd7cae3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dominant events: ['Speech', 'Music', '\"Chicken']\n"
          ]
        }
      ],
      "source": [
        "# This cell identifies dominant sound events by counting how often\n",
        "# each sound occurs across audio frames and selecting the most\n",
        "# frequently occurring events.\n",
        "from collections import Counter\n",
        "\n",
        "event_counter = Counter(frame_event_labels)\n",
        "\n",
        "# Minimum frames threshold (important for short sounds)\n",
        "MIN_FRAMES = 3\n",
        "\n",
        "dominant_events = [\n",
        "    event for event, count in event_counter.items()\n",
        "    if count >= MIN_FRAMES\n",
        "]\n",
        "\n",
        "# Fallback: strongest global event\n",
        "if not dominant_events:\n",
        "    mean_scores = np.mean(scores.numpy(), axis=0)\n",
        "    dominant_events = [class_names[np.argmax(mean_scores)]]\n",
        "\n",
        "print(\"Dominant events:\", dominant_events)\n",
        "\n",
        "#Runs YAMNet on the audio to detect sound events at the frame level."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B1ekr5qwNX4B"
      },
      "outputs": [],
      "source": [
        "# This cell defines a mapping from detected sound events\n",
        "# to meaningful English sentences used for audio caption generation.\n",
        "\n",
        "EVENT_TO_SENTENCE = {\n",
        "    # Animals\n",
        "    \"Cat\": \"A cat is meowing.\",\n",
        "    \"Dog\": \"A dog is barking.\",\n",
        "    \"Bird\": \"Birds are chirping.\",\n",
        "\n",
        "    # Musical instruments\n",
        "    \"Guitar\": \"A guitar is being played.\",\n",
        "    \"Piano\": \"A piano is playing.\",\n",
        "    \"Drum\": \"Drums are being played.\",\n",
        "    \"Violin\": \"A violin is playing.\",\n",
        "\n",
        "    # Vehicles\n",
        "    \"Vehicle horn\": \"A vehicle horn is sounding.\",\n",
        "    \"Car horn\": \"A car horn is honking.\",\n",
        "    \"Train horn\": \"A train horn is sounding.\",\n",
        "    \"Train\": \"A train horn is sounding.\",\n",
        "    \"Engine\": \"An engine sound is heard.\",\n",
        "\n",
        "    # Human\n",
        "    \"Speech\": \"A person is speaking.\",\n",
        "    \"Singing\": \"A person is singing.\",\n",
        "\n",
        "    # Environment\n",
        "    \"Rain\": \"Rain is falling.\",\n",
        "    \"Wind\": \"Wind is blowing.\",\n",
        "    \"Footsteps\": \"People are walking.\"\n",
        "}\n",
        "#Maps detected sound events to meaningful English sentences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6VuzmIaZNX7K"
      },
      "outputs": [],
      "source": [
        "# This cell defines a function to detect whether human speech\n",
        "# is present in the audio based on dominant sound events and\n",
        "# YAMNet confidence scores.\n",
        "\n",
        "def detect_speech(scores, class_names, dominant_events, threshold=0.5):\n",
        "    # Speech must be a dominant event\n",
        "    if \"Speech\" not in dominant_events:\n",
        "        return False\n",
        "\n",
        "    mean_scores = np.mean(scores.numpy(), axis=0)\n",
        "\n",
        "    speech_classes = [\"Speech\", \"Conversation\", \"Narration\"]\n",
        "    for cls in speech_classes:\n",
        "        if cls in class_names:\n",
        "            idx = class_names.index(cls)\n",
        "            if mean_scores[idx] > threshold:\n",
        "                return True\n",
        "\n",
        "    return False\n",
        "\n",
        "#Determines whether human speech is present in the audio using confidence scores."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O9erj_EdNX-Q"
      },
      "outputs": [],
      "source": [
        "# This cell defines the caption generation function that converts\n",
        "# dominant sound events into a complete human-readable sentence.\n",
        "\n",
        "def hierarchical_graphac_caption(dominant_events):\n",
        "    sentences = []\n",
        "    for event in dominant_events:\n",
        "        if event in EVENT_TO_SENTENCE:\n",
        "            sentences.append(EVENT_TO_SENTENCE[event])\n",
        "        else:\n",
        "            sentences.append(f\"The sound of {event.lower()} is heard.\")\n",
        "    return \" \".join(sentences)\n",
        "\n",
        "#Generates a caption by converting dominant sound events into a single sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V-FpUerONYA7"
      },
      "outputs": [],
      "source": [
        "# This cell defines the final decision logic that generates\n",
        "# the audio caption by using speech transcription if speech\n",
        "# is detected, otherwise using sound-based captioning.\n",
        "\n",
        "def final_caption(audio_path, scores, dominant_events, class_names):\n",
        "    background_caption = hierarchical_graphac_caption(dominant_events)\n",
        "\n",
        "    if detect_speech(scores, class_names, dominant_events):\n",
        "        result = whisper_model.transcribe(\n",
        "            audio_path,\n",
        "            task=\"translate\"\n",
        "        )\n",
        "        speech_text = result[\"text\"].strip()\n",
        "\n",
        "        if speech_text:\n",
        "            return (\n",
        "                f'A person says: \"{speech_text}\". '\n",
        "                f'Background sounds include: {background_caption}'\n",
        "            )\n",
        "\n",
        "    return background_caption\n",
        "\n",
        "\n",
        "#Selects between speech transcription and sound-based captioning to produce the final caption"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "LuuQ9zA9NYEA",
        "outputId": "f851e8db-3e8e-4495-a79d-6e62fc9f20db"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated Caption:\n",
            "A person says: \"Vanakkam, my name is Vivek. I am doing ASR research.\". Background sounds include: A person is speaking. The sound of cough is heard.\n"
          ]
        }
      ],
      "source": [
        "# This cell executes the complete audio captioning pipeline\n",
        "# and prints the final generated caption for the input audio.\n",
        "\n",
        "caption = final_caption(\n",
        "    audio_path,\n",
        "    scores,\n",
        "    dominant_events,\n",
        "    class_names\n",
        ")\n",
        "\n",
        "print(\"Generated Caption:\")\n",
        "print(caption)\n",
        "\n",
        "#Executes the full pipeline and prints the final generated audio caption"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
